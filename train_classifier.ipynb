{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tinctura/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import joblib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('task/dataset/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>is_fake</th>\n",
       "      <th>title_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Москвичу Владимиру Клутину пришёл счёт за вмеш...</td>\n",
       "      <td>1</td>\n",
       "      <td>москвич владимир клутина приша литр сч том вме...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Агент Кокорина назвал езду по встречке житейск...</td>\n",
       "      <td>0</td>\n",
       "      <td>агент кокорин назвать езда встречок житейский ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  is_fake  \\\n",
       "0  Москвичу Владимиру Клутину пришёл счёт за вмеш...        1   \n",
       "1  Агент Кокорина назвал езду по встречке житейск...        0   \n",
       "\n",
       "                                    title_normalized  \n",
       "0  москвич владимир клутина приша литр сч том вме...  \n",
       "1  агент кокорин назвать езда встречок житейский ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"russian\")\n",
    "stop_words.append(\"изза\")\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    \"\"\"Remove punctuation from given text\"\"\"\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "\n",
    "def remove_characters(text: str) -> str:\n",
    "    \"\"\"Remove characters from given text\"\"\"\n",
    "    return re.sub('[^а-яА-Я]', ' ', text)\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"Remove stop words from given text\"\"\"\n",
    "    result = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            result.append(token)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize each word in given text\"\"\"\n",
    "    normalized = []\n",
    "    for element in text.split():\n",
    "        normalized.append(morph.parse(element)[0].normal_form)\n",
    "    return \" \".join(normalized)\n",
    "\n",
    "\n",
    "def clear_text(text: str) -> str:\n",
    "    \"\"\"Cleans given text\"\"\"\n",
    "    text = text.lower()\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = normalize_text(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "data[\"title_normalized\"] = data[\"title\"].apply(clear_text)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['title_normalized']\n",
    "targets = data['is_fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 229 ms, sys: 22.2 ms, total: 251 ms\n",
      "Wall time: 258 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "word_vectorizer.fit(text)\n",
    "train_word_features = word_vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.9)\n",
    "clf.fit(train_word_features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9272261536332529"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score = np.mean(cross_val_score(clf, train_word_features, targets, cv=5, scoring='roc_auc'))\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39465699, 0.60534301]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_obj = \"Робот Фёдор пожаловался, что никто не поздравил его с Днём космонавтики\"\n",
    "vectorized_test_obj = word_vectorizer.transform([test_obj])\n",
    "clf.predict_proba(vectorized_test_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fake_news_vectorizer_dump.pkl', 'wb') as output_file:\n",
    "    joblib.dump(word_vectorizer, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fake_news_model_dump.pkl', 'wb') as output_file:\n",
    "    joblib.dump(clf, output_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
